{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2twHxPnZvO-"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import ast\n",
        "import gc\n",
        "import random\n",
        "import warnings\n",
        "import multiprocessing as mp\n",
        "from tqdm import tqdm\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoConfig,\n",
        "    get_cosine_schedule_with_warmup,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "ztM9i-TZaA6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read data"
      ],
      "metadata": {
        "id": "O_oLajjPc0OE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/fc_qa_dataset.csv')"
      ],
      "metadata": {
        "id": "0fkNAzkbbAl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline"
      ],
      "metadata": {
        "id": "F_f_-N0Zc16D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_common_questions(text):\n",
        "    common_questions = []\n",
        "    for ele in text.split('\\n'):\n",
        "        ele = ele.strip()\n",
        "        if ele.startswith('-'):\n",
        "            ele = ele.replace('-', '')\n",
        "            ele = ele.strip()\n",
        "            common_questions.append(ele)\n",
        "    return common_questions"
      ],
      "metadata": {
        "id": "idX812rEbmYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['common_questions'] = train['common_questions'].apply(get_common_questions)\n",
        "\n",
        "train['generated_questions'] = train['generated_questions'].apply(lambda x: ast.literal_eval(x))"
      ],
      "metadata": {
        "id": "2PtR2r5MbniU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_queries = train['common_questions'].sum()\n",
        "labels = (train['common_questions'].apply(len) * train.index.to_series().apply(lambda x: [x])).sum()"
      ],
      "metadata": {
        "id": "RQ68lhSAcNzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_top_k_accuracy(top_k_preds, true_labels):\n",
        "    binary_label_masks = []\n",
        "    for top_k_pred, true_label in zip(top_k_preds, true_labels):\n",
        "        if true_label in top_k_pred:\n",
        "            binary_label_masks.append(1)\n",
        "        else:\n",
        "            binary_label_masks.append(0)\n",
        "    accuracy = np.mean(binary_label_masks)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "H_kmG2RNHUYO"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_paths = [\n",
        "    'intfloat/multilingual-e5-small',\n",
        "    'intfloat/multilingual-e5-base',\n",
        "    'intfloat/multilingual-e5-large',\n",
        "    'bkai-foundation-models/vietnamese-bi-encoder',\n",
        "    'VoVanPhuc/sup-SimCSE-VietNamese-phobert-base'\n",
        "]\n",
        "\n",
        "for model_path in model_paths:\n",
        "    embed_model = SentenceTransformer(model_path)\n",
        "\n",
        "    desc_embed = embed_model.encode(\n",
        "        train['api_desc'],\n",
        "        batch_size=16,\n",
        "        device='cuda',\n",
        "        convert_to_tensor=True,\n",
        "        normalize_embeddings=True,\n",
        "        # show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    top_1_preds = []\n",
        "    top_3_preds = []\n",
        "    top_5_preds = []\n",
        "    for query in user_queries:\n",
        "        question_embed = embed_model.encode(query, device='cuda', convert_to_tensor=True, normalize_embeddings=True)\n",
        "        scores = question_embed @ desc_embed.T\n",
        "        top_1_pred = scores.argmax(dim=-1).cpu().numpy().tolist()\n",
        "        top_3_pred = scores.topk(3).indices.cpu().tolist()\n",
        "        top_5_pred = scores.topk(5).indices.cpu().tolist()\n",
        "\n",
        "        top_1_preds.append(top_1_pred)\n",
        "        top_3_preds.append(top_3_pred)\n",
        "        top_5_preds.append(top_5_pred)\n",
        "\n",
        "    top_1_acc = accuracy_score(top_1_preds, labels)\n",
        "    top_3_acc = calc_top_k_accuracy(top_3_preds, labels)\n",
        "    top_5_acc = calc_top_k_accuracy(top_5_preds, labels)\n",
        "\n",
        "    print(f'### {model_path}')\n",
        "    print(f'Accuracy@1: {top_1_acc}')\n",
        "    print(f'Accuracy@3: {top_3_acc}')\n",
        "    print(f'Accuracy@5: {top_5_acc}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuhlmagYcoEr",
        "outputId": "22dbeb2e-1732-48bf-d394-df4489fc38d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### intfloat/multilingual-e5-small\n",
            "Accuracy@1: 0.7805907172995781\n",
            "Accuracy@3: 0.8860759493670886\n",
            "Accuracy@5: 0.9282700421940928\n",
            "\n",
            "### intfloat/multilingual-e5-base\n",
            "Accuracy@1: 0.7383966244725738\n",
            "Accuracy@3: 0.890295358649789\n",
            "Accuracy@5: 0.9240506329113924\n",
            "\n",
            "### intfloat/multilingual-e5-large\n",
            "Accuracy@1: 0.7215189873417721\n",
            "Accuracy@3: 0.869198312236287\n",
            "Accuracy@5: 0.9324894514767933\n",
            "\n",
            "### bkai-foundation-models/vietnamese-bi-encoder\n",
            "Accuracy@1: 0.46835443037974683\n",
            "Accuracy@3: 0.7257383966244726\n",
            "Accuracy@5: 0.7890295358649789\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name VoVanPhuc/sup-SimCSE-VietNamese-phobert-base. Creating a new one with MEAN pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\n",
            "Accuracy@1: 0.37130801687763715\n",
            "Accuracy@3: 0.5189873417721519\n",
            "Accuracy@5: 0.6455696202531646\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning"
      ],
      "metadata": {
        "id": "BB_On9x9IN3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'model_name': 'intfloat/multilingual-e5-small',\n",
        "    'batch_size': 32,\n",
        "    'max_length': 512,\n",
        "    'epochs': 2,\n",
        "    'learning_rate': 2e-4,\n",
        "    'warmup_steps': 0,\n",
        "    'weight_decay': 0.1,\n",
        "    'intermediate_dropout': 0.,\n",
        "    'num_workers': mp.cpu_count(),\n",
        "    'seed': 252,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "}\n",
        "\n",
        "cfg = SimpleNamespace(**cfg)"
      ],
      "metadata": {
        "id": "-y1OKxMOJnnE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)"
      ],
      "metadata": {
        "id": "BAd3NZChLn4h"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VTNetDataset(Dataset):\n",
        "\tdef __init__(self, encodings_1, encodings_2):\n",
        "\t\tself.encodings_1 = encodings_1\n",
        "\t\tself.encodings_2 = encodings_2\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\titem = {f'{key}_1': torch.tensor(val[idx]) for key, val in self.encodings_1.items()}\n",
        "\t\titem.update(\n",
        "            {f'{key}_2': torch.tensor(val[idx]) for key, val in self.encodings_2.items()}\n",
        "        )\n",
        "\t\treturn item\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.encodings_1.input_ids.shape[0]"
      ],
      "metadata": {
        "id": "uzWJR576E8IX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(tokenizer, questions, descriptions, mode, batch_size, max_length, num_workers):\n",
        "\n",
        "\tencodings_1 = tokenizer(\n",
        "\t\tquestions,\n",
        "\t\tpadding='max_length',\n",
        "\t\ttruncation=True,\n",
        "\t\tmax_length=max_length,\n",
        "\t\treturn_tensors='pt'\n",
        "\t)\n",
        "\n",
        "\tencodings_2 = tokenizer(\n",
        "\t\tdescriptions,\n",
        "\t\tpadding='max_length',\n",
        "\t\ttruncation=True,\n",
        "\t\tmax_length=max_length,\n",
        "\t\treturn_tensors='pt'\n",
        "\t)\n",
        "\n",
        "\tdataset = VTNetDataset(encodings_1, encodings_2)\n",
        "\n",
        "\tif mode == 'train':\n",
        "\t\tdata_loader = DataLoader(\n",
        "\t\t\tdataset=dataset,\n",
        "\t\t\tbatch_size=batch_size,\n",
        "\t\t\tdrop_last=True,\n",
        "\t\t\tshuffle=True,\n",
        "\t\t\tnum_workers=num_workers\n",
        "\t\t)\n",
        "\n",
        "\telse:\n",
        "\t\tdata_loader = DataLoader(\n",
        "\t\t\tdataset=dataset,\n",
        "\t\t\tbatch_size=batch_size,\n",
        "\t\t\tdrop_last=False,\n",
        "\t\t\tshuffle=False,\n",
        "\t\t\tnum_workers=num_workers\n",
        "\t\t)\n",
        "\n",
        "\treturn data_loader"
      ],
      "metadata": {
        "id": "l39uzuqaHkxH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_questions = train['generated_questions'].sum()\n",
        "api_descriptions = (train['generated_questions'].apply(len) * train['api_desc'].apply(lambda x: [x])).sum()\n",
        "\n",
        "train_dataloader = get_dataloader(\n",
        "    tokenizer=tokenizer,\n",
        "    questions=api_questions,\n",
        "    descriptions=api_descriptions,\n",
        "    mode='train',\n",
        "    batch_size=cfg.batch_size,\n",
        "    max_length=cfg.max_length,\n",
        "    num_workers=cfg.num_workers,\n",
        ")"
      ],
      "metadata": {
        "id": "h1v40BwaJWdt"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=318):\n",
        "\trandom.seed(seed)\n",
        "\tnp.random.seed(seed)\n",
        "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\t# os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
        "\ttorch.manual_seed(seed)\n",
        "\ttorch.cuda.manual_seed(seed)\n",
        "\ttorch.backends.cudnn.deterministic = True\n",
        "\ttorch.backends.cudnn.benchmark = False\n",
        "\t# torch.use_deterministic_algorithms(True)"
      ],
      "metadata": {
        "id": "JJIXtMzRSk9R"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiNegativesRankingLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Ref: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py\n",
        "    \"\"\"\n",
        "    def __init__(self, scale=50):\n",
        "        super().__init__()\n",
        "        self.cross_entropy = nn.CrossEntropyLoss(reduction='mean')\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, embed_1, embed_2, labels=None):\n",
        "        cosine_scores = (\n",
        "            F.normalize(embed_1) @ F.normalize(embed_2).T\n",
        "        ) * self.scale\n",
        "\n",
        "        labels = torch.tensor(\n",
        "            range(len(cosine_scores)),\n",
        "            dtype=torch.long,\n",
        "            device=cosine_scores.device\n",
        "        )\n",
        "\n",
        "        loss = self.cross_entropy(cosine_scores, labels)\n",
        "        return loss\n",
        "\n",
        "\n",
        "loss_fn = MultiNegativesRankingLoss()"
      ],
      "metadata": {
        "id": "xjoxNRh_O90X"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextMeanPooling(nn.Module):\n",
        "    def __init__(self, eps=1e-06):\n",
        "        super(TextMeanPooling, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, token_embeddings, attention_mask):\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        mean_embeds = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=self.eps)\n",
        "        return mean_embeds"
      ],
      "metadata": {
        "id": "-pSZtjl5Q2SK"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VTNetEmbedModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super(VTNetEmbedModel, self).__init__()\n",
        "\n",
        "        config = AutoConfig.from_pretrained(cfg.model_name)\n",
        "        config.attention_probs_dropout_prob = cfg.intermediate_dropout\n",
        "        config.hidden_dropout_prob = cfg.intermediate_dropout\n",
        "\n",
        "        self.backbone = AutoModel.from_pretrained(cfg.model_name, config=config)\n",
        "        self.backbone.gradient_checkpointing_enable()\n",
        "\n",
        "        self.pooler = TextMeanPooling()\n",
        "        self.loss_fn = MultiNegativesRankingLoss()\n",
        "\n",
        "\n",
        "    def forward(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2):\n",
        "        embed_1 = self.backbone(input_ids_1, attention_mask_1).last_hidden_state\n",
        "        embed_2 = self.backbone(input_ids_2, attention_mask_2).last_hidden_state\n",
        "\n",
        "        x_1 = self.pooler(embed_1, attention_mask_1)\n",
        "        x_2 = self.pooler(embed_2, attention_mask_2)\n",
        "\n",
        "        loss = self.loss_fn(x_1, x_2)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "iKiuZ1C3QSda"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(cfg.seed)\n",
        "train_dataloader = get_dataloader(\n",
        "    tokenizer=tokenizer,\n",
        "    questions=api_questions,\n",
        "    descriptions=api_descriptions,\n",
        "    mode='train',\n",
        "    batch_size=cfg.batch_size,\n",
        "    max_length=cfg.max_length,\n",
        "    num_workers=cfg.num_workers,\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(cfg.epochs):\n",
        "\n",
        "    model = VTNetEmbedModel(cfg)\n",
        "    model.to(cfg.device)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=cfg.warmup_steps,\n",
        "        num_training_steps=len(train_dataloader)*cfg.epochs\n",
        "    )\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        input_ids_1 = batch['input_ids_1'].to(cfg.device)\n",
        "        attention_mask_1 = batch['attention_mask_1'].to(cfg.device)\n",
        "        input_ids_2 = batch['input_ids_2'].to(cfg.device)\n",
        "        attention_mask_2 = batch['attention_mask_2'].to(cfg.device)\n",
        "\n",
        "        with autocast():\n",
        "            loss = model(\n",
        "                input_ids_1=input_ids_1,\n",
        "                attention_mask_1=attention_mask_1,\n",
        "                input_ids_2=input_ids_2,\n",
        "                attention_mask_2=attention_mask_2\n",
        "            )\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        scheduler.step()\n",
        "\n",
        "        if not batch_idx % 10:\n",
        "            print(\n",
        "                f'Epoch: {epoch + 1}/{cfg.epochs}'\n",
        "                f' | Batch: {batch_idx}/{len(train_dataloader)}'\n",
        "                f' | Loss: {loss.detach().cpu().item():.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmN2G9GLSV21",
        "outputId": "a0a50bc8-2d4b-445d-854b-f48cf39bcbf0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/2 | Batch: 0/16 | Loss: 1.8027\n",
            "Epoch: 1/2 | Batch: 10/16 | Loss: 0.5589\n",
            "Epoch: 2/2 | Batch: 0/16 | Loss: 1.7778\n",
            "Epoch: 2/2 | Batch: 10/16 | Loss: 0.4032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ValDataset(Dataset):\n",
        "\tdef __init__(self, encodings):\n",
        "\t\tself.encodings = encodings\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\titem = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\t\treturn item\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.encodings['input_ids'].shape[0]\n",
        "\n",
        "\n",
        "encodings_1 = tokenizer(\n",
        "    user_queries,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=cfg.max_length,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "\n",
        "val_dataset_1 = ValDataset(encodings_1)\n",
        "val_dataloader_1 = DataLoader(\n",
        "    dataset=val_dataset_1,\n",
        "    batch_size=cfg.batch_size,\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=cfg.num_workers\n",
        ")\n",
        "\n",
        "\n",
        "encodings_2 = tokenizer(\n",
        "    train['api_desc'].values.tolist(),\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=cfg.max_length,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "\n",
        "val_dataset_2 = ValDataset(encodings_2)\n",
        "val_dataloader_2 = DataLoader(\n",
        "    dataset=val_dataset_2,\n",
        "    batch_size=cfg.batch_size,\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=cfg.num_workers\n",
        ")"
      ],
      "metadata": {
        "id": "7QcgRbf1VgiC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pooler = TextMeanPooling()\n",
        "desc_embed = torch.tensor([], device=cfg.device)\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for batch in val_dataloader_2:\n",
        "        input_ids = batch['input_ids'].to(cfg.device)\n",
        "        attention_mask = batch['attention_mask'].to(cfg.device)\n",
        "        embed = model.backbone(input_ids, attention_mask).last_hidden_state\n",
        "        mean_embed = pooler(embed, attention_mask)\n",
        "        mean_embed = F.normalize(mean_embed, dim=-1)\n",
        "        desc_embed = torch.cat((desc_embed, mean_embed), dim=0)"
      ],
      "metadata": {
        "id": "3DmxtS0lW1_f"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    top_1_preds = []\n",
        "    top_3_preds = []\n",
        "    top_5_preds = []\n",
        "    for batch in val_dataloader_1:\n",
        "        input_ids = batch['input_ids'].to(cfg.device)\n",
        "        attention_mask = batch['attention_mask'].to(cfg.device)\n",
        "        question_embed = model.backbone(input_ids, attention_mask).last_hidden_state\n",
        "        question_embed = pooler(question_embed, attention_mask)\n",
        "        question_embed = F.normalize(question_embed, dim=-1)\n",
        "        scores = question_embed @ desc_embed.T\n",
        "\n",
        "        top_1_pred = scores.argmax(dim=-1).cpu().numpy().tolist()\n",
        "        top_3_pred = scores.topk(3).indices.cpu().tolist()\n",
        "        top_5_pred = scores.topk(5).indices.cpu().tolist()\n",
        "\n",
        "        top_1_preds.extend(top_1_pred)\n",
        "        top_3_preds.extend(top_3_pred)\n",
        "        top_5_preds.extend(top_5_pred)\n",
        "\n",
        "top_1_acc = accuracy_score(top_1_preds, labels)\n",
        "top_3_acc = calc_top_k_accuracy(top_3_preds, labels)\n",
        "top_5_acc = calc_top_k_accuracy(top_5_preds, labels)\n",
        "\n",
        "print(f'Accuracy@1: {top_1_acc}')\n",
        "print(f'Accuracy@3: {top_3_acc}')\n",
        "print(f'Accuracy@5: {top_5_acc}')"
      ],
      "metadata": {
        "id": "hBAYUAPTYMs4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65ffb141-33e2-4f7d-b8cc-54ae20a85dd1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy@1: 0.8734177215189873\n",
            "Accuracy@3: 0.9662447257383966\n",
            "Accuracy@5: 0.9873417721518988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "77zhoV5WuaE_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}